# -*- coding: utf-8 -*-
"""Clustering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16qB3zPedR-NDYXbbjEHJZ8jHWshlWQcF

# 1.Library Import
"""

# Commented out IPython magic to ensure Python compatibility.
import os
# proxy is not needed for other computers
# os.environ['http_proxy'] = 'http://proxy-chain.intel.com:911'
# os.environ['https_proxy'] = 'http://proxy-chain.intel.com:912'

#!pip install openpyxl
#!pip3 install xgboost
#!pip install -U imbalanced-learn
#!pip install gensim
#!pip install nltk
#import nltk
#nltk.download('stopwords')
#nltk.download('punkt')
#!pip install sklearn
#!pip install langdetect

# Import required libraries
import numpy as np
import re #
import pandas as pd
# %matplotlib inline
import warnings


# Import required libraries for machine learning classifiers and NLP
from sklearn.preprocessing import OneHotEncoder
enc = OneHotEncoder(handle_unknown='ignore')
#try:
  #from catboost import CatBoostClassifier
#except:
  #!pip install catboost
  #from catboost import CatBoostClassifier

#!pip install kmodes
from kmodes.kmodes import KModes
import matplotlib.pyplot as plt
# %matplotlib inline



# Set random seed
np.random.seed(42)

warnings.filterwarnings("ignore")

#Measure run time
import time
start = time.time()

"""## 2. Data Upload"""

#Upload Safar's list of 193 tombs
#The source of the data must be changed according to where it is stored.
###################################################################################################
file = pd.read_csv(r'SafarTombs.csv')
###################################################################################################

#Create a copy of the data
df = file
len(df)

"""## 3. Feature Engineering
In this section, the original data is split and reconfigured in an adequate format for the model to ingest.
This includes:
- Data cleaning and wrangling
- Renaming of columns when needed
- Splitting columns with large descriptions into individual categories
- Selection of variables for analysis
- One-hot-encoding
- Creation of additional variables, including total number of burial items, total number of pottery types represented
"""

#Creates unique body ID for burials with multiple bodies
df['ID'] = df['Grave'].astype(str) + df['Individual']
df['Grave_ID'] = df['Grave'].astype(str)+"_"+df['ID'].astype(str)
df.set_index('Grave_ID', inplace=True)

#Creates variable for burial type
df['Type'] = df['Type'].str.lower()
df['Burial Type'] = pd.np.where(df['Type'].str.contains("libn box"), "Libn box", df['Type'])
df['Burial Type'] = pd.np.where(df['Type'].str.contains("floor"), "Libn floor", df['Burial Type'])
df['Burial Type'] = pd.np.where(df['Type'].str.contains("shaft"), "Shaft tomb", df['Burial Type'])
df['Burial Type'] = pd.np.where(df['Type'].str.contains("sand"), "In sand", df['Burial Type'])
df['Burial Type'] = pd.np.where(df['Type'].str.contains("debris"), "In debris", df['Burial Type'])
df.loc[pd.isnull(df['Type']), 'Burial Type'] = np.nan

#Clean size information
df['Size'] = df['Size'].replace('c.', "", regex=True).astype(float)

# Split Body position in separate phrases
def sentences(text):
    delimiters= ";"
    text = re.split(delimiters, str(text))
    clean_sent = []
    for sent in text:
        clean_sent.append(sent)
    return clean_sent

# sentence parsing creation
df1 = df['Position'].apply(sentences)
df1 = df['Position'].str.split(';', expand=True)
df1.columns = ['text1','text2','text3','text4','text5']

#Finds mentions of body position and copies text to appropiate columnn
############Find more pythonic way of doing this###########################################
names = ["body", "extended", "embryonic"]
df1['body'] = pd.np.where(df1['text1'].str.contains("|".join(names)), df1['text1'], np.nan)
df1['body'] = pd.np.where(df1['text2'].str.contains("|".join(names)), df1['text2'], df1['body'])
df1['body'] = pd.np.where(df1['text3'].str.contains("|".join(names)), df1['text3'], df1['body'])
df1['body'] = pd.np.where(df1['text4'].str.contains("|".join(names)), df1['text4'], df1['body'])
df1['body'] = pd.np.where(df1['text5'].str.contains("|".join(names)), df1['text5'], df1['body'])

#Finds mentions of hands and copies text to appropiate columnn
############Find more pythonic way of doing this###########################################
df1['hands'] = pd.np.where(df1['text1'].str.contains("hand"), df1['text1'], np.nan)
df1['hands'] = pd.np.where(df1['text2'].str.contains("hand"), df1['text2'], df1['hands'])
df1['hands'] = pd.np.where(df1['text3'].str.contains("hand"), df1['text3'], df1['hands'])
df1['hands'] = pd.np.where(df1['text4'].str.contains("hand"), df1['text4'], df1['hands'])
df1['hands'] = pd.np.where(df1['text5'].str.contains("hand"), df1['text5'], df1['hands'])

#Finds mentions of face and copies text to appropiate columnn
############Find more pythonic way of doing this###########################################
df1['face'] = pd.np.where(df1['text1'].str.contains("face"), df1['text1'], np.nan)
df1['face'] = pd.np.where(df1['text2'].str.contains("face"), df1['text2'], df1['face'])
df1['face'] = pd.np.where(df1['text3'].str.contains("face"), df1['text3'], df1['face'])
df1['face'] = pd.np.where(df1['text4'].str.contains("face"), df1['text4'], df1['face'])
df1['face'] = pd.np.where(df1['text5'].str.contains("face"), df1['text5'], df1['face'])

#Finds mentions of arms and copies text to appropiate columnn
############Find more pythonic way of doing this###########################################
df1['arm'] = pd.np.where(df1['text1'].str.contains("arm"), df1['text1'], np.nan)
df1['arm'] = pd.np.where(df1['text2'].str.contains("arm"), df1['text2'], df1['arm'])
df1['arm'] = pd.np.where(df1['text3'].str.contains("arm"), df1['text3'], df1['arm'])
df1['arm'] = pd.np.where(df1['text4'].str.contains("arm"), df1['text4'], df1['arm'])
df1['arm'] = pd.np.where(df1['text5'].str.contains("arm"), df1['text5'], df1['arm'])

#Finds mentions of skulls and copies text to appropiate columnn
############Find more pythonic way of doing this###########################################
names = ["skull", "head"]
df1['skull'] = pd.np.where(df1['text1'].str.contains("|".join(names)), df1['text1'], np.nan)
df1['skull'] = pd.np.where(df1['text2'].str.contains("|".join(names)), df1['text2'], df1['skull'])
df1['skull'] = pd.np.where(df1['text3'].str.contains("|".join(names)), df1['text3'], df1['skull'])
df1['skull'] = pd.np.where(df1['text4'].str.contains("|".join(names)), df1['text4'], df1['skull'])
df1['skull'] = pd.np.where(df1['text5'].str.contains("|".join(names)), df1['text5'], df1['skull'])

#Finds mentions of legs and copies text to appropiate columnn
############Find more pythonic way of doing this###########################################
names = ["leg", "knee"]
df1['leg'] = pd.np.where(df1['text1'].str.contains("|".join(names)), df1['text1'], np.nan)
df1['leg'] = pd.np.where(df1['text2'].str.contains("|".join(names)), df1['text2'], df1['leg'])
df1['leg'] = pd.np.where(df1['text3'].str.contains("|".join(names)), df1['text3'], df1['leg'])
df1['leg'] = pd.np.where(df1['text4'].str.contains("|".join(names)), df1['text4'], df1['leg'])
df1['leg'] = pd.np.where(df1['text5'].str.contains("|".join(names)), df1['text5'], df1['leg'])

df = pd.concat([df, df1[['body','hands','face','arm','skull','leg']]], axis=1).drop(['Position'], axis=1)

df.columns

#Separates multiple items into dummies with unique pottery types
df2 = df[['Pottery types']]
df2['Pottery types'] = df2['Pottery types'].str.replace('\d{3}', "", regex=True)
df2 = df2['Pottery types'].str.get_dummies(sep=';')
df2.columns = df2.columns.str.strip()
#reverse dummies into columns
df2 = df2.replace(1, pd.Series(df2.columns, df2.columns))
df2 = df2.applymap(lambda x: 1 if isinstance(x, str) else x)

df_pottery=df2.join(df[['Level']])
df_pottery.to_csv("pottery_sample.csv")

#Separates multiple items into dummies with unique body positions and burial types
df3 = df[['body','hands','face','arm','skull','leg', 'Burial Type']]
#One-Hot-Encoding for body positions
df4 = pd.get_dummies(data=df3, columns=['body','hands','face','arm','skull','leg', 'Burial Type'])
df4.columns = df4.columns.str.strip()
#reverse dummies into columns
df4 = df4.replace(1, pd.Series(df4.columns, df4.columns))

# Join all datasets
dataset = df2.join(df4).join(df[['Grave', 'Individual', 'Level', 'Orientation', 'Type', 'Sex', 'Age',
       'Size', 'Other objects']])
dataset = dataset.replace(np.nan, '').astype(str)

dataset

"""Four different samples will be created:
- Full burial sample (the full dataframe)
- Top sub-level only
- Lower sublevel only
- Data from new excavations"""

#Top level data only:
#top = df.loc[df['Sub-Level'] == '0.50']
#low = df.loc[df['Sub-Level'] == '1.00']
#new=

#samples = [top, low, new]

"""## 4. Clustering Models
Focused on algorithms specific to categorical data or mixed data

## K-Modes
"""

data =  dataset.copy()
# Elbow curve to find optimal K - I tried both with random init and Cao init
cost = []
K = range(1,5)
for num_clusters in list(K):
    kmode = KModes(n_clusters=num_clusters, init = "random", n_init = 5, verbose=1)
    kmode.fit_predict(data)
    cost.append(kmode.cost_)

plt.plot(K, cost, 'bx-')
plt.xlabel('No. of clusters')
plt.ylabel('Cost')
plt.title('Elbow Method For Optimal k')
plt.show()

"""Configuration of all the models to test and application to each sample"""
# Building the model with 2 clusters
kmode = KModes(n_clusters=2, init = "random", n_init = 5, verbose=1)
clusters = kmode.fit_predict(data)

"""Definition of optimal number of clusters with loop for each sample"""
#for x in samples:

data.insert(0, "Cluster", clusters, True)

#Prepare for evaluation of feature importance
cols = df2.columns
data[cols] = np.where(data[cols] =='0', 0, 1)

cols2 = df4.columns
data[cols2] = np.where(data[cols2] =='0', 0, 1)

len(data)

data['Cluster'].value_counts()

#reverse dummies into columns
table = pd.pivot_table(data, index=['Cluster'], aggfunc='sum')
cluster0 = table.iloc[0]
cluster0 = cluster0.sort_values(ascending=False)
cluster1 = table.iloc[1]
cluster1 = cluster1.sort_values(ascending=False)

diff0 = cluster0.nlargest(50).index.difference(cluster1.nlargest(30).index)
diff1 = cluster1.nlargest(30).index.difference(cluster0.nlargest(30).index)

diff0

#, facecolor='w', edgecolor='k')
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 5))
fig.suptitle('Top-40 contributing features to clusters')
cluster0.nlargest(50).plot(ax=ax1, kind='bar')
cluster1.nlargest(50).plot(ax=ax2, kind='bar')
ax1.title.set_text('Cluster 0 Unique characteristics')
ax2.title.set_text('Cluster 1 Unique characteristics')

"""## K-Prototypes
First published by Huang (1998)
"""

#data[]

#Preprocessing numerical
numerical = data.select_dtypes(exclude='object')
numerical

"""## 6. Visualization of results"""

end = time.time()
print(end - start)