# -*- coding: utf-8 -*-
"""Clustering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16qB3zPedR-NDYXbbjEHJZ8jHWshlWQcF

# 1.Library Import
"""

# Commented out IPython magic to ensure Python compatibility.
#import os
# proxy is not needed for other computers
#os.environ['http_proxy'] = 'http://proxy-chain.intel.com:911'
#os.environ['https_proxy'] = 'http://proxy-chain.intel.com:912'
# explicitly require this experimental feature
from sklearn.experimental import enable_iterative_imputer  # noqa
from fancyimpute import KNN
from sklearn.preprocessing import OrdinalEncoder
from sklearn.metrics import silhouette_score
#!pip install fancyimpute
from sklearn.cluster import DBSCAN
#!pip install openpyxl
#!pip3 install xgboost
#!pip install -U imbalanced-learn
#!pip install gensim
#!pip install nltk
#import nltk
#nltk.download('stopwords')
#nltk.download('punkt')
#!pip install sklearn
#!pip install langdetect
import datetime
# Import required libraries
import numpy as np
import re #
import pandas as pd
# %matplotlib inline
import warnings
from sklearn.cluster import KMeans
from kmodes.kmodes import KModes
from kmodes.kprototypes import KPrototypes

# Import required libraries for machine learning classifiers and NLP
from sklearn.preprocessing import OneHotEncoder
enc = OneHotEncoder(handle_unknown='ignore')
#try:
  #from catboost import CatBoostClassifier
#except:
  #!pip install catboost
  #from catboost import CatBoostClassifier

#!pip install kmodes
from kmodes.kmodes import KModes
import matplotlib.pyplot as plt
# %matplotlib inline


# Package imports
import seaborn as sns
import pandas as pd
import missingno as msno
#%matplotlib inline

# Set random seed
np.random.seed(42)

warnings.filterwarnings("ignore")

#Measure run time
import time
start = time.time()

"""## 2. Data Upload"""

#Upload Safar's list of 193 tombs
#The source of the data must be changed according to where it is stored.
###################################################################################################
file = pd.read_csv(r'SafarTombs_rev.csv')
file_2022 = pd.read_csv(r'Tombs Eridu 2022.csv')
###################################################################################################

#Create a copy of the data
df = file.copy()
df_2022 = file_2022.copy()
df_2022 = df_2022.head(10)
len(df)


###############################################################################
#Addign the new data 2022 to safar tombs
###############################################################################
df_2022 = df_2022.replace('nan', np.nan)
df_2022 = df_2022.replace('nan', '')
columns = ["Position Body", "Position Hands", "Position arms", "Position head", "Position leg"]
df_2022['Position'] = df_2022[columns].astype(str).apply(lambda x: ' '.join(y for y in x if y != 'nan'), axis=1)

# Eliminar espacios extra
df_2022['Position'] = df_2022['Position'].str.replace(' +', ' ').str.strip()

columnas_comunes = df.columns.intersection(df_2022.columns)

# Seleccionar solo las columnas comunes del segundo DataFrame
df_2022_select = df_2022[columnas_comunes]

# Agregar las filas seleccionadas al primer DataFrame
df = df.append(df_2022_select)


###############################################################################
#Missingno- Getting the missing values on the dataframe
###############################################################################
def missing_values(df, doc_name):
    missing_values = df.isnull().sum()
    percentage_missing_per_column = 100-((missing_values / len(df)) * 100)
    
    # Visualizar los datos faltantes
    msno.matrix(df)
    msno.bar(df)
    
    percentage_df = pd.DataFrame(percentage_missing_per_column, columns=['% Non-Missing'])
    
    # Guardar los gr√°ficos
    plt.figure()
    msno.matrix(df)
    plt.savefig(f'Results\Missignno\{doc_name}_missing_matrix.png')
    plt.show()
    
    plt.figure()
    msno.bar(df)
    plt.savefig(f'Results\Missignno\{doc_name}_missing_bar.png')
    plt.show()
    
    
    # Guardar el DataFrame
    percentage_df.to_csv(f'Results\Missignno\{doc_name}_percentage_missing.csv')
missing_values(df, "Safar_Tombs")

"""## 3. Feature Engineering
In this section, the original data is split and reconfigured in an adequate format for the model to ingest.
This includes:
- Data cleaning and wrangling
- Renaming of columns when needed
- Splitting columns with large descriptions into individual categories
- Selection of variables for analysis
- One-hot-encoding
- Creation of additional variables, including total number of burial items, total number of pottery types represented
"""
def clean_text(text):
    if pd.isna(text):
        return text
    # Remove extra whitespace
    clean_text = ' '.join(text.split())

    # Remove line breaks and other special control characters
    clean_text = clean_text.replace('\n', ' ')
    clean_text = clean_text.replace('\r', '')
    clean_text = clean_text.lower()
    clean_text = clean_text.strip()
    return clean_text

group_by= df.groupby('Grave')
# Para ver el resultado puedes hacer:
df['N_Individuals'] = df.groupby('Grave')['Grave'].transform('count')

#Creates unique body ID for burials with multiple bodies
df['ID'] = df['Grave'].astype(str) + df['Individual']
df['Grave_ID'] = df['Grave'].astype(str)+"_"+df['ID'].astype(str)
df.set_index('Grave_ID', inplace=True)


#Creates unique body ID for burials with multiple bodies
df_2022['ID'] = df_2022['Grave'].astype(str) + df_2022['Individual']
df_2022['Grave_ID'] = df_2022['Grave'].astype(str)+"_"+df_2022['ID'].astype(str)
df_2022.set_index('Grave_ID', inplace=True)

# Rellenar los valores faltantes en 'Level', 'Orientation' y 'Type' con el valor anterior en el mismo 'Grave'
for column in ['Level', 'Orientation', 'Type']:
    df[column] = df.groupby('Grave')[column].ffill()

df['Level'] = df['Level'].replace('Surface', 0)


#Creates variable for burial type
df['Type'] = df['Type'].str.lower()
df['Burial Type'] = pd.np.where(df['Type'].str.contains("libn box"), "Libn box", df['Type'])
df['Burial Type'] = pd.np.where(df['Type'].str.contains("libn floor"), "Libn floor", df['Burial Type'])
df['Burial Type'] = pd.np.where(df['Type'].str.contains("platform"), "Libn floor", df['Burial Type'])
df['Burial Type'] = pd.np.where(df['Type'].str.contains("clay floor"), "Clay floor", df['Burial Type'])
df['Burial Type'] = pd.np.where(df['Type'].str.contains("shaft"), "Shaft tomb", df['Burial Type'])
df['Burial Type'] = pd.np.where(df['Type'].str.contains("sand"), "In sand", df['Burial Type'])
df['Burial Type'] = pd.np.where(df['Type'].str.contains("debris"), "In debris", df['Burial Type'])

df['Burial Type'] = pd.np.where(df['Type'].str.contains("found beneath another burial|in'ubaid deposit|much desintegrated|over an older burial|over an older grave"), np.nan , df['Burial Type'])
df.loc[pd.isnull(df['Type']), 'Burial Type'] = np.nan


#Clean size information
df['Size'] = df['Size'].replace('c.', "", regex=True).astype(float)

Count_Individual_BurialType = df.groupby('Burial Type')['Individual'].count()
Count_Individual_BurialType.to_csv(r"Results/Samples/Count_Individual_BurialType.csv")

def category_body(text):
    words_to_replace = {
        ######################### body #########################################
        'extended on back': [
            'extended on back',
            'extended on the back',
            'extended',
            'skeleton extended on back',
            'body laid on back',
            'body extended',
            'body on back',
            'extended on back',
            'on back',
            'originally extended on back',
            'originally extended',
        ],
        'body to the right': [
            'body slightly turned on right side',
            'body to the right',
            'extended slightly on the right side',
            'body placed on the right side',
            'body slightly on the right',
            'body extended on right side',
            'extended on right side',
            'body placed on right side'
        ],
        'body to the left': [
            'body slightly turned on left side',
            'body to the left',
            'body on left side',
            'extended slightly on the left side',
            'body placed on the left side',
            'body slightly on the left',
            'body extended on left side',
            'extended on left side',
            'body on the left side',
            'body placed on left side'
        ],
    
   ######################### hands #########################################
       'hands near pelvis': [
           'hands on legs',
           'hands below pelvis',
           'hands on pelvis',
           'hands near pelvis',
           'hand near pelvis',
       ],
       'hands beside legs': [
           'hands extended',
           'hands beside legs',
           'hands by the sides',
           'hands at side',
           'hands at sides',
           'hands straight',
       ],
       'hands crossed on pelvis': [
           'hands one over another near the pelvis',
           'hands one over another near pelvis',
           'hands meeting at the pelvis',
           'hands meeting at pelvis',
       ],
       'hands upwards': [
           'upwards hands'
       ],
       'hand near chin': [
           'right hand near chin',
           'left hand touching chin'
       ],
       'hands on chest': [
           'hands on chest',
           'right hands on chest',
       ],
       'hand on head': [
           'right hand on her head',
       ],
       
       'right hand near pelvis': [
           'right hand on pelvis',
           'right hand near pelvis',
           'right hand over pelvis',
           'right hand flexed on pelvis',
       ],
       'right hand beside leg': [
           'right hand extended',
           'right hand at side',
           'right hand straight',
       ],
       
       'left hand near pelvis': [
           'left hand on pelvis',
           'left hand-on pelvis',
           'left hand on pelvic',
           'left hand near pelvis',
           'left hand on pelvis orientaded nw',
       ],
       'left hand beside leg': [
           'left hand extended',
           'left hand at side',
           'left hand beside leg',
           'left hand straight',
       ],
       ######################### Arms #########################################
       'arms in disorder': [
           'arms in disorder',
       ],
       'left arm flexed': [
           'left arm flexed',
           'left arm bent',
           'left arm near pelvis',
       ],
       'right arm flexed': [
           'right arm flexed',
           'right arm flexed on abdomen',
           'right arm flexed on body',
           'right arm bent on body',
           'right arm bent over the body',
           'right arm bent over the chest',
       ],
       'right arm straight': [
           'right arm straight',
       ],
       'left arm straight': [
           'right arm straight',
       ],
       'arms straight': [
           'arms straight',
           'arms extended by the sides',
       ],
       ######################### face #########################################
        'face upward': [
            'face',
            'face upward',
            'face upward covered with a large fragment of Ubaid bowl',
        ],
        'face eastward': [
            'face eastward',
            'face slightly eastward',
            'face slightly eastwards',
        ],
        'face westward': [
            'face westward',
            'face slightly westward',
            'face slightly westwards',
            'head slightly westward',
            'head facing west',
        ],
        ######################### skull #########################################
        'skull collapsed': [
            'skull collapsed',
            'head collapsed',
            'skull crushed',
            'head crushed',
            'skull fallen',
            'head fallen',
            'skull smashed',
            'head smashed',
            'skull collapsed, displaced',
            'smashed skull',
        ],
        'skull missing': [
            'skull missing',
            'head missing',
            'face missing',
            'skull, shoulder and part of the chest are missing due to an old pit',
        ],
        'skull to the right': [
            'skull found on the right side of this burial',
        ],
        'skull to the left': [
            'skull found on the left to the female skeleton',
            'skull east to the left arm',
        ],
        ######################### legs #########################################
        'legs flexed': [
            'legs slightly bent',
            'legs slightly in desorder',
            'legs flexed',
            'right slightly in flexed',
            'legs bent backward',
            'legs flexed at knees',
            'legs bent westward',
            'knees flexed',
            'knees bent',
            'knees probably bent',
            'knees slightly bent',
            'knees slightly flexed',
            'legs slightly flexed',
            'right knee slightly flexed',
            'right leg slightly flexed',
            'legs flexed so that feet look backward',
        ],
        'legs crossed': [
            'legs crossed',
        ],
        'legs crossed at feet': [
            'legs crossed at feet',
        ],
        'legs extended': [
            'legs extended',
            'legs are extended',
            'legs straight',
            'legs extended but not in line with the spine',
        ],
        'legs missing': [
            'legs missing due to pit cut',
        ],
        'legs in disorder': [
            'legs in disorder',
        ],
        ######################### Position Confused #########################################
        '': [
            'position confused',
        ],
       }
    if pd.isna(text):
        return text

    array_text = text.split(';')
    modified_array = []

    for item in array_text:
        item_lower = clean_text(item)
        replaced = False
        for category, words in words_to_replace.items():
            for word in words:
                if item_lower == word.lower():
                    modified_array.append(category)
                    replaced = True
                    break  # Salir del bucle interno si hay una coincidencia
            if replaced:
                break  # Salir del bucle externo si se hizo una sustituci√≥n

        else:
            modified_array.append(item)

    joined_text = ';'.join(modified_array)

    return joined_text

df['Position'] = df['Position'].apply(category_body)
df['Position']

# Split Body position in separate phrases
def sentences(text):
    delimiters= ";"
    text = re.split(delimiters, str(text))
    clean_sent = []
    for sent in text:
        clean_sent.append(sent)
    return clean_sent

# sentence parsing creation
df1 = df['Position'].apply(sentences)
df1 = df['Position'].str.split(';', expand=True)
df1.columns = ['text1','text2','text3','text4','text5','text6']

#Finds mentions of body position and copies text to appropiate columnn
############Find more pythonic way of doing this###########################################
names = ["body to", "extended on", "embryonic"]
df1['body'] = pd.np.where(df1['text1'].str.contains("|".join(names)), df1['text1'], np.nan)
df1['body'] = pd.np.where(df1['text2'].str.contains("|".join(names)), df1['text2'], df1['body'])
df1['body'] = pd.np.where(df1['text3'].str.contains("|".join(names)), df1['text3'], df1['body'])
df1['body'] = pd.np.where(df1['text4'].str.contains("|".join(names)), df1['text4'], df1['body'])
df1['body'] = pd.np.where(df1['text5'].str.contains("|".join(names)), df1['text5'], df1['body'])
df1['body'] = pd.np.where(df1['text5'].str.contains("|".join(names)), df1['text6'], df1['body'])

#Finds mentions of hands and copies text to appropiate columnn
############Find more pythonic way of doing this###########################################
df1['hands'] = pd.np.where(df1['text1'].str.contains("hand"), df1['text1'], np.nan)
df1['hands'] = pd.np.where(df1['text2'].str.contains("hand"), df1['text2'], df1['hands'])
df1['hands'] = pd.np.where(df1['text3'].str.contains("hand"), df1['text3'], df1['hands'])
df1['hands'] = pd.np.where(df1['text4'].str.contains("hand"), df1['text4'], df1['hands'])
df1['hands'] = pd.np.where(df1['text5'].str.contains("hand"), df1['text5'], df1['hands'])
df1['hands'] = pd.np.where(df1['text5'].str.contains("hand"), df1['text6'], df1['hands'])


#Finds mentions of arms and copies text to appropiate columnn
############Find more pythonic way of doing this###########################################
df1['arm'] = pd.np.where(df1['text1'].str.contains("arm"), df1['text1'], np.nan)
df1['arm'] = pd.np.where(df1['text2'].str.contains("arm"), df1['text2'], df1['arm'])
df1['arm'] = pd.np.where(df1['text3'].str.contains("arm"), df1['text3'], df1['arm'])
df1['arm'] = pd.np.where(df1['text4'].str.contains("arm"), df1['text4'], df1['arm'])
df1['arm'] = pd.np.where(df1['text5'].str.contains("arm"), df1['text5'], df1['arm'])
df1['arm'] = pd.np.where(df1['text5'].str.contains("arm"), df1['text6'], df1['arm'])

#Finds mentions of skulls and copies text to appropiate columnn
############Find more pythonic way of doing this###########################################
names = ["skull", "head", "face"]
df1['head'] = pd.np.where(df1['text1'].str.contains("|".join(names)), df1['text1'], np.nan)
df1['head'] = pd.np.where(df1['text2'].str.contains("|".join(names)), df1['text2'], df1['head'])
df1['head'] = pd.np.where(df1['text3'].str.contains("|".join(names)), df1['text3'], df1['head'])
df1['head'] = pd.np.where(df1['text4'].str.contains("|".join(names)), df1['text4'], df1['head'])
df1['head'] = pd.np.where(df1['text5'].str.contains("|".join(names)), df1['text5'], df1['head'])
df1['head'] = pd.np.where(df1['text5'].str.contains("|".join(names)), df1['text6'], df1['head'])

#Finds mentions of legs and copies text to appropiate columnn
############Find more pythonic way of doing this###########################################
names = ["leg", "knee"]
df1['leg'] = pd.np.where(df1['text1'].str.contains("|".join(names)), df1['text1'], np.nan)
df1['leg'] = pd.np.where(df1['text2'].str.contains("|".join(names)), df1['text2'], df1['leg'])
df1['leg'] = pd.np.where(df1['text3'].str.contains("|".join(names)), df1['text3'], df1['leg'])
df1['leg'] = pd.np.where(df1['text4'].str.contains("|".join(names)), df1['text4'], df1['leg'])
df1['leg'] = pd.np.where(df1['text5'].str.contains("|".join(names)), df1['text5'], df1['leg'])
df1['leg'] = pd.np.where(df1['text5'].str.contains("|".join(names)), df1['text6'], df1['leg'])

df1 = df1.replace(r'^\s*$', np.nan, regex=True)

df = pd.concat([df, df1[['body','hands','arm','head','leg']]], axis=1).drop(['Position'], axis=1)

df.columns


count_dummie=df[['body','hands','arm','head','leg']]
count_dummies = pd.get_dummies(data=count_dummie, columns=['body','hands','arm','head','leg'], dummy_na=True)
# Suma de todas las columnas de dummies





#Separates multiple items into dummies with unique pottery types
df2 = df[['Pottery types']]

patron1 = r'\b\d{1,3}[A-Za-z]\b'
patron2 = r'\b\d{2}\b'

# Combine patron1 and patron2 using the | symbol for "or"
patron = f'{patron1}|{patron2}'
finds_match = []
other_objects_match = []

# Iterar a trav√©s de la columna 'Texto'
for texto in df2['Pottery types']:
    if pd.isna(texto):
        finds_match.append(texto)
        other_objects_match.append(texto)
    else:
        coincidencias_finds = re.findall(patron, texto)
        texto_modificado = re.sub(patron, '', texto)
        texto_modificado = re.sub("pottery types:", '', texto_modificado)
        texto_modificado = re.sub("pottery types", '', texto_modificado)
        texto_modificado = re.sub(r'[^\w\s]', '', texto_modificado)
        other_objects_match.append(texto_modificado)
        if len(coincidencias_finds) != 0:
            finds_match.append(';'.join(coincidencias_finds))
        else:
            finds_match.append(float('nan'))
        
df2["Pottery types"] = finds_match
# Inserta el valor en el DataFrame
df2.at['4_2020_4_2020A', 'Pottery types'] = '80;5A;13E'

df_pottery=df2.copy()
papure_dumies = df_pottery['Pottery types'].str.get_dummies(sep=';')

df["N_Parure"] = papure_dumies.sum(axis=1)


# crear columna de categor√≠as de 

pottery_arrangement_dic={
    'jar': [
        '2a','2b',
        '3a','3b',
        '4a','4b',
        
        '9a','9b','9c',
        '13a','13b','13c','13d','13e',
        '14','21','22','23','31',
        
        '10a','10b',
        '12a','12b','12c',
        '15a','15b',
        
        '16a','16b',
        '18a','18b',
        
        '28a','28b',
        '29a','29b',
        '30a','30b',
        '32a','32b',
    ],
    'cup': [
        '1a','1b','1c',
        '11a','11b',
        '19a','19b','19c',
        '80',
    ],
    'bowl': [
        '6a','6b','6c','6d',
        '7a','7b','7c','7d',
        '8a','8b','8c',
        '20a','20b',
        '24a','24b',
        '25a','25b',
        '26a','26b','26c','26d','26e',
        '27a','27b','27c','27d','27e',
        '36','52',
        

    ],
    'dish': [
        '5a','5b','5c','5d',
        '33a','33b','33c',
    ],
    
    }

category_service = ['cup', 'bowl', 'dish']

# Invierte el diccionario para mapear los tipos de cer√°mica a sus categor√≠as
pottery_type_to_category = {pottery: category for category, potteries in pottery_arrangement_dic.items() for pottery in potteries}

# Funci√≥n para determinar la funci√≥n de la cer√°mica
def determine_function(category):
    return 'storage' if category == 'jar' else 'service'

df_pottery['Comb Set'] = ''
df_pottery['Pottery Functions'] = ''
# Recorre el DataFrame con un bucle for
for index, row in df_pottery.iterrows():
    types = row['Pottery types']
    if pd.isnull(types):  # Verifica si types es NaN
        continue  # Si es NaN, salta a la siguiente iteraci√≥n
    
    array_text = types.split(';')
    modified_array = []
    modified_array2 = []
    for item in array_text:
        item_lower = clean_text(item)
        replaced = False
        for category, words in pottery_arrangement_dic.items():
            for word in words:
                if item_lower == word.lower():
                    modified_array.append(category)
                    if category in category_service:
                        modified_array2.append("service")
                    if category == "jar":
                        modified_array2.append("storage")
                    replaced = True
                    break  # Salir del bucle interno si hay una coincidencia
            if replaced:
                break  # Salir del bucle externo si se hizo una sustituci√≥n

        else:
            modified_array.append(item)

    joined_text = ';'.join(modified_array)
    joined_text2 = ';'.join(modified_array2)
    
    df_pottery.at[index, 'Comb Set'] = joined_text
    df_pottery.at[index, 'Pottery Functions'] = joined_text2
    
df_pottery['Comb Set']
# Divide la cadena en la columna 'Pottery Function' en palabras individuales
df_pottery['Comb Set'] = df_pottery['Comb Set'].str.split(';')

# Crea una serie con todos los tipos de cer√°mica
all_types = pd.Series([type.strip() for sublist in df_pottery['Comb Set'].dropna() for type in sublist])

# Cuenta la ocurrencia de cada tipo de cer√°mica
type_counts = all_types.value_counts()

# Crea un nuevo DataFrame con la estructura deseada
Pottery_Class = pd.DataFrame({'Type': type_counts.index, 'number': type_counts.values})

Pottery_Class.to_csv(r"Results/Samples/Pottery_Class.csv")



# Divide la cadena en la columna 'Pottery Function' en palabras individuales
df_pottery['Pottery Functions'] = df_pottery['Pottery Functions'].str.split(';')

# Crea una serie con todos los tipos de cer√°mica
all_types = pd.Series([type.strip() for sublist in df_pottery['Pottery Functions'].dropna() for type in sublist])

# Cuenta la ocurrencia de cada tipo de cer√°mica
type_counts = all_types.value_counts()

# Crea un nuevo DataFrame con la estructura deseada
Pottery_Function = pd.DataFrame({'Type': type_counts.index, 'number': type_counts.values})

Pottery_Function.to_csv(r"Results/Samples/Pottery_Functions.csv")




# Recorre el DataFrame con un bucle for
for index, row in df_pottery.iterrows():
    
    comb_set = row['Comb Set']
    pottery_functions = row['Pottery Functions']
    
    if "" == comb_set[0]:  # Verifica si types es NaN
        # Actualiza las celdas en el DataFrame
        df_pottery.at[index, 'Comb Set'] = np.nan
        df_pottery.at[index, 'Pottery Functions'] = np.nan
        continue  # Si es NaN, salta a la siguiente iteraci√≥n
    
    # Cuenta la cantidad de cada tipo de cer√°mica y su funci√≥n
    comb_set_counts = {i: comb_set.count(i) for i in comb_set}
    pottery_functions_counts = {i: pottery_functions.count(i) for i in pottery_functions}
    
    # Convierte los diccionarios de recuentos en cadenas y los ordena alfab√©ticamente
    comb_set_str = ';'.join(sorted(f'{k} {v}' for k, v in comb_set_counts.items()))
    pottery_functions_str = ';'.join(sorted(f'{k} {v}' for k, v in pottery_functions_counts.items()))
    
    # Actualiza las celdas en el DataFrame
    df_pottery.at[index, 'Comb Set'] = comb_set_str
    df_pottery.at[index, 'Pottery Functions'] = pottery_functions_str





# preparacion de dataset que se utiliza para analisis de correspondencia
df2_nan = pd.DataFrame()
df2_nan['nan'] = df2.isnull().any(axis=1).astype(int)

df2 = df2['Pottery types'].str.get_dummies(sep=';')
df2 = pd.concat([df2, df2_nan], axis=1)

df2 = df2.add_prefix("Pottery_")

df2.columns = df2.columns.str.strip()
#reverse dummies into columns
df2 = df2.replace(1, pd.Series(df2.columns, df2.columns))
df2 = df2.applymap(lambda x: 1 if isinstance(x, str) else x)

df_pottery_count = df2.copy()
df_pottery_count_overlap = df2.copy()
df2=df2.join(df[['Level']])

df_pottery_count = df_pottery_count.join(df[['Level']])
df_pottery_count = df_pottery_count.groupby('Level').sum().reset_index()

df_pottery_count.to_csv("pottery_sample.csv")

df_pottery_count_overlap = df_pottery_count_overlap.join(df[['Overlap']])
df_pottery_count_overlap = df_pottery_count_overlap.groupby('Overlap').sum().reset_index()
df_pottery_count_overlap.to_csv("pottery_overlap_sample.csv")

# Separar los valores en la columna 'Pottery types'
#df_pottery['Pottery types'] = df_pottery['Pottery types'].apply(lambda x: x.split(';') if pd.notna(x) and isinstance(x, str) else [])

##get dummies for Other objects
def category_other_objects(text):
    words_to_replace = {
        'meat bone': [
            'meat bone',
            'meat-bone',
            'meal bone',
            'meat bones',
        ],
        'fish bone': [
            'fish',
            'fish bone',
            'fish bones',
        ],
       'stone bead': [
           'stone beads',
           'bead of rock crystal',
           'green stone',
           'frit beads',
           'read stone',
           'black beads',
           'beads',
       ],
       'mat': [
           'with a mat',
       ],
       'necklace': [
           'necklace',
       ],
       'obsidian': [
           'obsidian',
       ],
       'ochre': [
           'ochre-paint',
       ],
       'animal remains': [
           'animal jaw',
           'skull of an animal',
       ],
       'clay pellet': [
           'clay pellet',
       ],
       'stone dish': [
           'stone dish',
       ],
        }
    if pd.isna(text):
        return text

    array_text = text.split(';')
    modified_array = []
    for item in array_text:
        item_lower = item.lower()
        replaced = False
        for category, words in words_to_replace.items():
            for word in words:
                if word.lower() in item_lower:
                    modified_array.append(category)
                    replaced = True
                    break  # Salir del bucle interno si hay una coincidencia
            if replaced:
                break  # Salir del bucle externo si se hizo una sustituci√≥n

        else:
            modified_array.append(item)

    joined_text = ';'.join(modified_array)

    return joined_text

df['Objects'] = df['Other objects'].apply(category_other_objects)
df['Objects']


def location_other_objects(text):
    words_to_replace = {
       'breast': [
           'on chest',
           'on breast',
       ],
       'head': [
           'near head',
           'beneath skull',
           'of the head',
       ],
       'jaw': [
           'lower jaw',
           'both sides of the jaw',
       ],
       'box': [
           'on box',
       ],
       'pelvis': [
           'near pelvis',
           'near hips',
           'around hips',
           'round hips',
       ],
       'leg': [
           'left leg',
           'around knees',
       ],
       'dish': [
           'dish',
       ],
       'neck': [
           'side of neck',
       ],
       'feet': [
           'right foot',
       ],
        }
    if pd.isna(text):
        return text

    array_text = text.split(';')
    modified_array = []
    for item in array_text:
        item_lower = item.lower()
        replaced = False
        for category, words in words_to_replace.items():
            for word in words:
                if word.lower() in item_lower:
                    modified_array.append(category)
                    replaced = True
                    break  # Salir del bucle interno si hay una coincidencia
            if replaced:
                break  # Salir del bucle externo si se hizo una sustituci√≥n

        else:
            modified_array.append(item)

    joined_text = ';'.join(modified_array)

    return joined_text

df['Objects Location'] = df['Other objects'].apply(location_other_objects)

def number_other_objects(text):
    words_to_replace = {
        '1': ['two', '2'],
        '2': ['three', '3'],
        '3': ['four', '4']
    }
    if pd.isna(text):
        return 0  # Devuelve 0 si el texto es NaN

    array_text = text.split(';')
    count = len(array_text)  # Conteo inicial de elementos en el array
    for item in array_text:
        item_lower = item.lower()
        for category, words in words_to_replace.items():
            for word in words:
                if word.lower() in item_lower:
                    count += int(category)  # Suma el n√∫mero correspondiente al conteo

    return count  # Devuelve solo el conteo total

df['N_Objects'] = df['Other objects'].apply(number_other_objects)
df['N_Parure'] = df['N_Objects'] + df['N_Parure']


sex_dummies = pd.get_dummies(df['Sex'], prefix='Sex', dummy_na=True)
sex_dummies = sex_dummies.join(df[['Level']])
sex_dummies = sex_dummies.set_index('Level')
sex_dummies = sex_dummies.groupby('Level').sum().reset_index()
sex_dummies = sex_dummies.set_index('Level')
# Obt√©n dummies de la columna 'Sex'

sex_dummies.to_csv("level_sex_sample.csv")

df["Level"]=df2["Level"]
df2=df2.drop('Level', axis=1)

df_pottery_comb_set = df_pottery['Comb Set'].copy()

df_pottery_comb_set = pd.get_dummies(df_pottery_comb_set, prefix='Comb_Set_', dummy_na=True)
df2 = df2.join(df_pottery_comb_set)

def create_dummies(df, columns):
    #Separates multiple items into dummies with unique body positions and burial types
    df_dummies = df[columns]
    #One-Hot-Encoding for body positions

    df_dummies = pd.get_dummies(data=df_dummies, columns=columns, dummy_na=True)
    return df_dummies


###############################################################################
#Imputation of Sex, Age and Size
###############################################################################

columns_impute = ['Sex', 'Age', 'Level']
columns_impute_dummies = ["Orientation", 'Burial Type', 'body','hands','arm','head','leg', 'Objects', 'Objects Location']
columns_to_join = ['Overlap', 'N_Individuals', 'N_Parure', 'N_Objects']

impute_dummies = create_dummies(df, columns_impute_dummies)

df_impute1 = df2.join(impute_dummies).join(df[columns_to_join]).join(df[columns_impute])
df_impute1['Level'] = df_impute1['Level'].astype(str)

df_impute1['Level'] = df_impute1['Level'].replace('nan',np.nan)
df_impute1


#instantiate both packages to use
encoder = OrdinalEncoder()
imputer = KNN()
# create a list of categorical columns to iterate over
cat_cols = columns_impute

encoders = {}  # Almacena un encoder para cada columna

def encode(data, column):
    '''function to encode non-null data and replace it in the original data'''
    # retains only non-null values
    nonulls = np.array(data.dropna())
    # reshapes the data for encoding
    impute_reshape = nonulls.reshape(-1,1)
    # encode data
    encoder = OrdinalEncoder()
    impute_ordinal = encoder.fit_transform(impute_reshape)
    # store the fitted encoder
    encoders[column] = encoder
    # Assign back encoded values to non-null values
    data.loc[data.notnull()] = np.squeeze(impute_ordinal)
    return data

# create a for loop to iterate through each column in the data
for column in cat_cols:
    encode(df_impute1[column], column)


# impute data and convert 
df_encoded = pd.DataFrame(np.round(imputer.fit_transform(df_impute1)),columns = df_impute1.columns)
df_encoded

# decode the columns 
df_decoded = df_encoded.copy()
for column in cat_cols:
    df_decoded[column] = encoders[column].inverse_transform(df_encoded[column].values.reshape(-1, 1))
df_decoded

df.reset_index(inplace=True)
df_decoded.reset_index(inplace=True)

df[['Sex', 'Age', 'Level']] = df_decoded[['Sex', 'Age', 'Level']]
df.reset_index(inplace=True)
df.set_index('Grave_ID', inplace=True)

df

###############################################################################
#Dummies for dataset
###############################################################################

columns1 = ["Orientation", 'Burial Type', "Sex", "Age", 'body','hands','arm','head','leg', 'Objects', 'Objects Location']

columns2 = ["Orientation", 'Burial Type', "Sex", "Age", 'body','hands','arm','head','leg']

columns3 = ['Burial Type', "Age", 'body','hands','arm','head','leg']

df1_dummies = create_dummies(df, columns1)
df2_dummies = create_dummies(df, columns2)
df3_dummies = create_dummies(df, columns3)

###############################################################################
#Drop repeated dummies
###############################################################################
def drop_columns(df, columns_to_drop):
    df_dropped = df.drop(columns_to_drop, axis=1)
    return df_dropped

columns_to_drop = ['leg_hands beside legs', 'leg_left hand beside leg', 'leg_lower jaw east of the left leg', 
                   'leg_right hand beside leg', 'arm_skull east of the left arm']

df1_dummies = drop_columns(df1_dummies, columns_to_drop)
df2_dummies = drop_columns(df2_dummies, columns_to_drop)
df3_dummies = drop_columns(df3_dummies, columns_to_drop)

df1_dummies.columns = df1_dummies.columns.str.strip()
df2_dummies.columns = df2_dummies.columns.str.strip()
df3_dummies.columns = df3_dummies.columns.str.strip()

df['Overlap'] = df['Overlap'].replace(np.nan, 0.0)
df['Overlap'] = df['Overlap'].replace('', 0.0)


df_original = df.copy()
###############################################################################
# Join all datasets
###############################################################################
def join_and_clean_datasets(df, df2, df_dummies, columns_to_join):
    # Unir todos los conjuntos de datos
    
    dataset = df2.join(df_dummies).join(df[columns_to_join])

    # Reemplazar los valores NaN y convertir el DataFrame a string
    dataset = dataset.replace(np.nan, '').astype(str)

    # Reemplazar los valores 'Surface', NaN y '' en la columna 'Level' por 0
    dataset['Level'] = dataset['Level'].replace('Surface', 0.0)
    dataset['Level'] = dataset['Level'].replace(np.nan, 0.0)
    dataset['Level'] = dataset['Level'].replace('', 0.0)
    

    return dataset

#all the columns -Size
columns_to_join = ['Level', 'Overlap', 'N_Individuals', 'N_Parure', 'N_Objects']
dataset1 = join_and_clean_datasets(df, df2, df1_dummies, columns_to_join)

columns_to_join = ['Level', 'N_Individuals', 'N_Parure', 'N_Objects']
dataset2 = join_and_clean_datasets(df, df2, df2_dummies, columns_to_join)

dataset3 = join_and_clean_datasets(df, df2, df3_dummies, columns_to_join)


###############################################################################
# Save Datasets
###############################################################################
def save_dataset(dataset, name):
    import os
    # Crear el nombre del archivo
    filename = f"{name}.csv"

    # Crear la ruta de la carpeta
    folder_path = os.path.join("Results\Datasets", name)
    # Ruta completa del archivo
    full_path = os.path.join(folder_path, filename)
    # Guardar el DataFrame como un archivo CSV
    dataset.to_csv(full_path)

save_dataset(dataset1, "DataSet1")
save_dataset(dataset2, "DataSet2")
save_dataset(dataset3, "DataSet3")

dataset1 = pd.read_csv(r'Results\DataSets\DataSet1\DataSet1.csv')
dataset1.set_index('Grave_ID', inplace=True)
dataset2 = pd.read_csv(r'Results\DataSets\DataSet2\DataSet2.csv')
dataset2.set_index('Grave_ID', inplace=True)
dataset3 = pd.read_csv(r'Results\DataSets\DataSet3\DataSet3.csv')
dataset3.set_index('Grave_ID', inplace=True)

dataset_list = [dataset1, dataset2, dataset3]

###############################################################################
############## CHI-SQUARE TEST USING SCIPY.STATS.CHI2_CONTINGENC ##############
###############################################################################

def chi_squares(dataset, df_original):
    from scipy import stats
    
    data = dataset.copy()
    cols_to_drop = data.filter(regex='^Age', axis=1).columns
    data = data.drop(columns=cols_to_drop)
    data["Age"] = df_original["Age"]
    columns_without_age = data.columns.drop('Age')
    data_crosstab = pd.crosstab(data['Age'], [data[col] for col in columns_without_age])
        # Realizar la prueba de chi-cuadrado
    chi2, p_value, dof, expected = stats.chi2_contingency(data_crosstab)

    print("Estad√≠stico de chi-cuadrado:", chi2)
    print("Valor p:", p_value)
    print("Grados de libertad:", dof)


chi_squares(dataset3, df_original)


###############################################################################
############################## Elbow method ###################################
###############################################################################


def find_elbow(K, distortions):
    # Coordenadas de la l√≠nea que conecta el primer y √∫ltimo punto
    coords = np.vstack((K, distortions)).T
    # Distancia de cada punto a la l√≠nea
    dist = np.abs(np.cross(coords[-1]-coords[0], coords[0]-coords))/np.linalg.norm(coords[-1]-coords[0])
    # El 'codo' es el punto con la mayor distancia a la l√≠nea
    elbow = dist.argmax() + 1  # +1 porque los √≠ndices en Python empiezan en 0
    return elbow

def kmeans_elbow_method(index, data, max_clusters):
    distortions = []
    K = range(1, max_clusters+1)

    for num_clusters in list(K):
        kmeanModel = KMeans(n_clusters=num_clusters, init='k-means++', random_state=42)
        kmeanModel.fit(data)
        distortions.append(kmeanModel.inertia_)

    # Encontrar el 'codo'
    elbow = find_elbow(K, distortions)
    # Graficar el m√©todo del codo
    plt.figure()
    plt.plot(K, distortions, 'bx-')
    plt.xlabel('k')
    plt.ylabel('Distortion')
    plt.title(f'The Elbow Method for DF{index} KMeans showing the optimal k')
    file_name = f"DF{index} Elbow"
    plt.savefig(f"Results\DataSets\DataSet{index}\Predictions\{file_name}_KMeans.png")
    plt.show()

    return elbow


def kmodes_elbow_method(index, data, max_clusters):
    distortions = []
    K = range(1, max_clusters+1)

    for num_clusters in list(K):
        kmodeModel = KModes(n_clusters=num_clusters, init='random', random_state=42)
        kmodeModel.fit(data)
        distortions.append(kmodeModel.cost_)

    # Encontrar el 'codo'
    elbow = find_elbow(K, distortions)

    # Graficar el m√©todo del codo
    plt.figure()
    plt.plot(K, distortions, 'bx-')
    plt.xlabel('k')
    plt.ylabel('Distortion')
    plt.title(f'The Elbow Method for DF{index} KModes showing the optimal k')
    file_name = f"DF{index} Elbow"
    plt.savefig(f"Results\DataSets\DataSet{index}\Predictions\{file_name}_KModes.png")
    plt.show()

    return elbow

    
def kprototypes_elbow_method(index, dataset, numerical_columns1, numerical_columns2, end):
    start=1
    # Selecciona las columnas num√©ricas bas√°ndose en el √≠ndice
    if index > 1:
        numerical_columns = numerical_columns2
    else:
        numerical_columns = numerical_columns1

    # Definir las columnas num√©ricas
    X_num = dataset[numerical_columns]

    # Definir las columnas categ√≥ricas
    X_cat = dataset.drop(numerical_columns, axis=1)

    # Unir los conjuntos de datos num√©ricos y categ√≥ricos
    X = pd.concat([X_num, X_cat], axis=1)

    # Crear una lista de √≠ndices de columnas categ√≥ricas
    cat_columns_idx = [X.columns.get_loc(col) for col in X_cat.columns]

    no_of_clusters = list(range(1, end+1))
    cost_values = []
    
    for k in no_of_clusters:
        test_model = KPrototypes(n_clusters=k, init='Cao', random_state=42)
        test_model.fit_predict(X, categorical=cat_columns_idx)
        cost_values.append(test_model.cost_)
        
    elbow = find_elbow(no_of_clusters,cost_values)
    plt.figure()
    sns.set_theme(style="whitegrid", palette="bright", font_scale=1.2)
    ax = sns.lineplot(x=no_of_clusters, y=cost_values, marker="o", dashes=False)
    ax.set_title(f'The Elbow Method for DF{index} KProto showing the optimal k')
    ax.set_xlabel('No of clusters', fontsize=14)
    ax.set_ylabel('Cost', fontsize=14)
    ax.set(xlim=(start-0.1, end+0.1))
    file_name = f"DF{index} Elbow"
    plt.plot()
    plt.savefig(f"Results\DataSets\DataSet{index}\Predictions\{file_name}_KProto.png")
    plt.show()
    
    return elbow
    
numerical_columns1 = ['Level', 'Overlap', 'N_Individuals', 'N_Parure', 'N_Objects']
numerical_columns2 = ['Level',  'N_Individuals', 'N_Parure', 'N_Objects']

###############################################################################
################################## Silhouette #################################
###############################################################################


def kmeans_silhouette_method(index, dataset, max_clusters):
    from yellowbrick.cluster import KElbowVisualizer
    model = KMeans(init='k-means++', random_state=42)
    visualizer = KElbowVisualizer(model, k=(2,max_clusters+1),metric='silhouette', timings= True)
    visualizer.fit(dataset)     
    optimal_k = visualizer.elbow_value_ 
    file_name = f"DF{index} Silhouette"
    plt.savefig(f"Results\DataSets\DataSet{index}\Predictions\{file_name}_KMeanss.png")
    visualizer.show()  
    return optimal_k



def kmodes_silhouette_method(index, dataset, max_clusters):
    silhouette_scores = []
    K = range(2, max_clusters+1)  # Silhouette no est√° definido para k=1

    for num_clusters in list(K):
        kmodeModel = KModes(n_clusters=num_clusters, init='random', random_state=42)
        labels = kmodeModel.fit_predict(dataset)
        silhouette_scores.append(silhouette_score(dataset, labels))

    # Encontrar el 'codo'
    elbow = find_elbow(K, silhouette_scores)

    # Graficar los puntajes de silueta
    plt.figure()
    plt.plot(K, silhouette_scores, 'bx-')
    plt.xlabel('k')
    plt.ylabel('Silhouette Score')
    plt.title(f'Silhouette Score Elbow for DF{index} KModes showing the optimal k')
    file_name = f"DF{index} Silhouette"
    plt.savefig(f"Results\DataSets\DataSet{index}\Predictions\{file_name}_KModes.png")
    plt.show()

    return elbow




def kprototypes_silhouette_method(index, dataset, numerical_columns1, numerical_columns2, max_clusters):
    silhouette_scores = []
    K = range(2, max_clusters+1)  # Silhouette no est√° definido para k=1

    # Selecciona las columnas num√©ricas bas√°ndose en el √≠ndice
    if index > 1:
        numerical_columns = numerical_columns2
    else:
        numerical_columns = numerical_columns1
        
    

    # Definir las columnas num√©ricas
    X_num = dataset[numerical_columns]

    # Definir las columnas categ√≥ricas
    X_cat = dataset.drop(numerical_columns, axis=1)

    # Unir los conjuntos de datos num√©ricos y categ√≥ricos
    X = pd.concat([X_num, X_cat], axis=1)

    # Crear una lista de √≠ndices de columnas categ√≥ricas
    cat_columns_idx = [X.columns.get_loc(col) for col in X_cat.columns]

    for num_clusters in list(K):
        kprotoModel = KPrototypes(n_clusters=num_clusters, init='Cao', random_state=42)
        labels = kprotoModel.fit_predict(X, categorical=cat_columns_idx)
        silhouette_scores.append(silhouette_score(dataset, labels))

    # Encontrar el 'codo'
    elbow = find_elbow(K, silhouette_scores)

    # Graficar los puntajes de silueta
    plt.figure()
    plt.plot(K, silhouette_scores, 'bx-')
    plt.xlabel('k')
    plt.ylabel('Silhouette Score')
    plt.title(f'Silhouette Score Elbow for DF{index} KPrototypes showing the optimal k')
    file_name = f"DF{index} Silhouette"
    plt.savefig(f"Results\DataSets\DataSet{index}\Predictions\{file_name}_KPrototypes.png")
    plt.show()

    return elbow


###############################################################################
################################ Gap Statics ##################################
###############################################################################
def kmeans_gap_method(data, maxClusters):
    nrefs=3
    gaps = np.zeros((len(range(1, maxClusters)),))
    resultsdf = pd.DataFrame({'clusterCount':[], 'gap':[]})
    for gap_index, k in enumerate(range(1, maxClusters)):

        # Holder for reference dispersion results
        refDisps = np.zeros(nrefs)

        # For n references, generate random sample and perform kmeans getting resulting dispersion of each loop
        for i in range(nrefs):
            
            # Create new random reference set
            randomReference = np.random.random_sample(size=data.shape)
            # Fit to it
            km = KMeans(k)
            km.fit(randomReference)
            
            refDisp = km.inertia_
            refDisps[i] = refDisp
            
        # Fit cluster to original data and create dispersion
        km = KMeans(k)
        km.fit(data)
        
        origDisp = km.inertia_

        # Calculate gap statistic
        gap = np.log(np.mean(refDisps)) - np.log(origDisp)

        # Assign this loop's gap statistic to gaps
        gaps[gap_index] = gap
        
        resultsdf = resultsdf.append({'clusterCount':k, 'gap':gap}, ignore_index=True)

    return (gaps.argmax() + 1, resultsdf)  # Plus 1 because index of 0 means 1 cluster is optimal, index 2 = 3 clusters are optimal


#k, gapdf = optimalK(dataset, maxClusters=5)



###############################################################################
################################ Clustering ###################################
###############################################################################
"""Configuration of all the models to test and application to each sample"""

# Method for KMeans
def kmeans_model(n_clusters, index, dataset, numerical_columns1, numerical_columns2):
    # Initialize KMeans model with specified number of clusters
    kmeans = KMeans(n_clusters=n_clusters, init='k-means++', random_state=42)
    
    # Fit the model to the data and predict the cluster labels
    kmeans_clusters = kmeans.fit_predict(dataset)
    
    # Create a copy of the dataset to avoid modifying the original
    dataset_kmeans = dataset.copy()
    
    # Add the cluster labels as a new column in the dataset
    dataset_kmeans['Cluster'] = kmeans_clusters
    
    if index > 1:
        numerical_columns = numerical_columns2
    else:
        numerical_columns = numerical_columns1
    
    ncat_dummies = create_dummies(dataset_kmeans, numerical_columns)
    dataset_kmeans = dataset_kmeans.join(ncat_dummies)
    level_dummies = pd.get_dummies(dataset_kmeans['Level'], prefix="Level")
    dataset_kmeans.drop(numerical_columns, axis=1, inplace=True)
    dataset_kmeans.drop(level_dummies, axis=1, inplace=True)
    dataset_kmeans = dataset_kmeans.loc[:, ~dataset_kmeans.columns.str.endswith('_nan')]
    
    return dataset_kmeans



# Method for KModes
def kmodes_model(n_clusters, index, dataset, numerical_columns1, numerical_columns2):
    # Initialize KModes model with specified number of clusters
    kmode = KModes(n_clusters=n_clusters, init = "random", random_state=42)
    
    # Fit the model to the data and predict the cluster labels
    kmode_clusters = kmode.fit_predict(dataset)
    
    # Create a copy of the dataset to avoid modifying the original
    dataset_kmodes = dataset.copy()
    
    # Add the cluster labels as a new column in the dataset
    dataset_kmodes['Cluster'] = kmode_clusters
    if index > 1:
        numerical_columns = numerical_columns2
    else:
        numerical_columns = numerical_columns1
    
    ncat_dummies = create_dummies(dataset_kmodes, numerical_columns)
    dataset_kmodes = dataset_kmodes.join(ncat_dummies)
    level_dummies = pd.get_dummies(dataset_kmodes['Level'], prefix="Level")
    dataset_kmodes.drop(numerical_columns, axis=1, inplace=True)
    dataset_kmodes.drop(level_dummies, axis=1, inplace=True)
    dataset_kmodes = dataset_kmodes.loc[:, ~dataset_kmodes.columns.str.endswith('_nan')]

    
    return dataset_kmodes



def kprototype_model(n_clusters, index, dataset, numerical_columns1, numerical_columns2):
    if index > 1:
        numerical_columns = numerical_columns2
    else:
        numerical_columns = numerical_columns1

    # Definir las columnas num√©ricas
    X_num = dataset[numerical_columns]

    # Definir las columnas categ√≥ricas
    X_cat = dataset.drop(numerical_columns, axis=1)
    
    # Unir los conjuntos de datos num√©ricos y categ√≥ricos
    X = pd.concat([X_num, X_cat], axis=1)
    
    # Crear una lista de √≠ndices de columnas categ√≥ricas
    cat_columns_idx = [X.columns.get_loc(col) for col in X_cat.columns]
    
    kproto = KPrototypes(n_clusters=n_clusters, init='Cao', random_state=42)
    
    kproto_clusters = kproto.fit_predict(X, categorical=cat_columns_idx)
    
    # Create a copy of the dataset to avoid modifying the original
    dataset_kproto = dataset.copy()
    
    # Add the cluster labels as a new column in the dataset
    dataset_kproto['Cluster'] = kproto_clusters
    
    ncat_dummies = create_dummies(dataset_kproto, numerical_columns)
    dataset_kproto = dataset_kproto.join(ncat_dummies)
    level_dummies = pd.get_dummies(dataset_kproto['Level'], prefix="Level")
    dataset_kproto.drop(numerical_columns, axis=1, inplace=True)
    dataset_kproto.drop(level_dummies, axis=1, inplace=True)
    dataset_kproto = dataset_kproto.loc[:, ~dataset_kproto.columns.str.endswith('_nan')]

    return dataset_kproto




def show_dbscan_clusters(X, clusters, index):
    df = pd.DataFrame(dict(x=X[:,0], y=X[:,1], label=clusters))
    colors = {-1:'red',0:'blue',1:'orange',2:'green',3:'skyblue'}
    fig, ax = plt.subplots(figsize=(8,8)) 
    grouped = df.groupby('label')
    for key, group in grouped:
        group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color= colors[key])
    plt.xlabel('X_1')
    plt.ylabel('X_2')
    plt.title(f'DBSCAN grups for DF{index}')
    file_name = f"DF{index}"
    plt.savefig(f"Results\DataSets\DataSet{index}\{file_name}_dbscan_clusters.png")
    plt.show()
    
def dbscan_model(dataset, eps, min_samples, index, numerical_columns1, numerical_columns2):
    from sklearn.decomposition import PCA
    db = DBSCAN(eps=eps, min_samples=min_samples).fit(dataset)
    clusters = db.labels_
    # Aplicar PCA y reducir a 2 dimensiones
    pca = PCA(n_components=2)
    dataset_pca = pca.fit_transform(dataset)
    show_dbscan_clusters(dataset_pca, clusters, index)
    dataset["Cluster"] = clusters+1
    if index > 1:
        numerical_columns = numerical_columns2
    else:
        numerical_columns = numerical_columns1
    
    ncat_dummies = create_dummies(dataset, numerical_columns)
    dataset = dataset.join(ncat_dummies)
    level_dummies = pd.get_dummies(dataset['Level'], prefix="Level")
    dataset.drop(numerical_columns, axis=1, inplace=True)
    dataset.drop(level_dummies, axis=1, inplace=True)
    dataset = dataset.loc[:, ~dataset.columns.str.endswith('_nan')]
    
    return dataset





def create_result_dictionary(dataset_list):
    x = len(dataset_list)

    # Crear un diccionario vac√≠o para almacenar los datos
    data_dict = {}

    # Definir la estructura de cada dataset
    dataset_structure = {
        'kmodes': {
            'elbow': None,
            'silhouette': None,
            'gap_stat': None
        },
        'kmeans': {
            'elbow': None,
            'silhouette': None,
            'gap_stat': None
        },
        'kprototypes': {
            'elbow': None,
            'silhouette': None,
            'gap_stat': None
        },
    }

    # A√±adir x cantidad de datasets al diccionario
    for i in range(x):
        data_dict[f'dataset{i+1}'] = dataset_structure
        
    return data_dict
    
prediction_dictionary = create_result_dictionary(dataset_list)
cluster_dataset_dictionary = create_result_dictionary(dataset_list)
prediction_dictionary

dbscan_cluster3 = dbscan_model(dataset3.copy(), 3, 38     ,3,  numerical_columns1, numerical_columns2)
dbscan_cluster2 = dbscan_model(dataset2.copy(), 3.7, 93     ,2, numerical_columns1, numerical_columns2)
dbscan_cluster1 = dbscan_model(dataset1.copy(), 3.7, 90    ,1, numerical_columns1, numerical_columns2)






def get_unique_caracteristics(dataset, df_name, method_name, key, index):
    data= dataset.copy()
    # Cuenta los valores en la columna 'Cluster'
    cluster_counts = data['Cluster'].value_counts()
    # Crea una tabla din√°mica para sumar los valores de cada cluster
    table = pd.pivot_table(data, index=['Cluster'], aggfunc='sum')
    
    # Crea una lista para guardar los DataFrames de cada cluster
    clusters = []
    
    # Ordena y guarda los valores de cada cluster
    for i in range(len(cluster_counts)):
        cluster = table.iloc[i].sort_values(ascending=False)
        clusters.append(cluster)
    
    differences = []
    
    # Iterar sobre cada cluster
    for i in range(len(clusters)):
        # Unir los clusters restantes
        other_clusters = pd.concat([clusters[j] for j in range(len(clusters)) if i != j])
        
        # Encontrar las diferencias y a√±adirlas a la lista
        diff = clusters[i].nlargest(np.count_nonzero(clusters[i])).index.difference(other_clusters.nlargest(np.count_nonzero(other_clusters)).index)
        differences.append(diff)
    
    # Calcular el n√∫mero de clusters
    n_clusters = len(clusters)
    if n_clusters > 2:
        he =  n_clusters * 15
    else:
        he =   35 
    
    # Crear una figura y un conjunto de subgr√°ficos
    fig, axs = plt.subplots(1,n_clusters, figsize=( he, 30))
    
    # Establecer el t√≠tulo de la figura
    fig.suptitle(f'Top Unique Contributing Features for {df_name} {method_name}-{key}')
    
    # Iterar sobre cada cluster
    for i in range(n_clusters):
        # Ordenar las caracter√≠sticas por valor y crear un gr√°fico de barras horizontales
        clusters[i][differences[i]].sort_values(ascending=True).plot(ax=axs[i], kind='barh')
        
        # Establecer el t√≠tulo del subgr√°fico
        axs[i].set_title(f'Cluster {i+1} Unique Characteristics')
        
        # Rotar las etiquetas del eje x
        axs[i].set_xticklabels(axs[i].get_xticklabels(), rotation=90)
    
    # Ajustar el dise√±o
    plt.tight_layout()
    file_name = f"DF{index} Unique Char"
    plt.savefig(f"Results\DataSets\DataSet{index}\\Unique_features\{file_name}_{method_name}_{key}.png")
    plt.show()


# i=0
# for method, metrics in cluster_dataset_dictionary.items():
#     i=i+1
#     for metric, dataset in metrics.items():
#         if dataset is not None:
#             for key, df in dataset.items():
#                 if df is not None and 'Cluster' in df.columns:
#                     get_unique_caracteristics(df, method, metric, key, i)

get_unique_caracteristics(dbscan_cluster3, "Dataset3", "DBSCAN", "", 3)
get_unique_caracteristics(dbscan_cluster2, "Dataset2", "DBSCAN", "", 2)
get_unique_caracteristics(dbscan_cluster1, "Dataset1", "DBSCAN", "", 1)



def get_contrib_caracteristics(dataset, df_name, method_name, key, index):
    data= dataset.copy()
    # Cuenta los valores en la columna 'Cluster'
    cluster_counts = data['Cluster'].value_counts()

    # Crea una tabla din√°mica para sumar los valores de cada cluster
    table = pd.pivot_table(data, index=['Cluster'], aggfunc='sum')
    clusters = []
    minimum_index = 1000
    for i in range(len(cluster_counts)):
        cluster = table.iloc[i].sort_values(ascending=False)
        
        cluster = cluster.nlargest(np.count_nonzero(cluster))
        if minimum_index > np.count_nonzero(cluster):
            minimum_index = np.count_nonzero(cluster)
        clusters.append(cluster)

    # Crea una lista para guardar los √≠ndices de cada cluster
    indices = [cluster.index for cluster in clusters]
    # Usa la funci√≥n reduce de la biblioteca functools para encontrar la intersecci√≥n de todos los √≠ndices
    from functools import reduce
    common_index = reduce(lambda a, b: a.intersection(b), indices)
    # Crear un diccionario para almacenar los datos de cada cluster
    data_clusters = {}
    
    # Iterar sobre cada cluster
    for i in range(len(clusters)):
        # A√±adir los datos del cluster al diccionario
        data_clusters[f'Cluster{i}'] = clusters[i][common_index]#common_index - clusters[i].index
        
    
    # Crear un DataFrame con los datos de los clusters
    data_cluster = pd.DataFrame(data_clusters)
    # Obt√©n los nombres de las columnas (es decir, los nombres de los clusters)
    cluster_names = data_cluster.columns.tolist()
    
    # Ordena los datos en orden descendente
    data_cluster = data_cluster.sort_values(by=cluster_names, ascending=True)
    
    # Selecciona solo las primeras 40 filas
    data_cluster = data_cluster.iloc[:len(data_cluster)]
    
    # Ordena los datos en orden ascendente
    data_cluster = data_cluster.sort_values(by=cluster_names, ascending=True)
    
    plt.figure()
    # Create the stacked bar chart
    ax = data_cluster.plot.barh(stacked=True, figsize=(20, 25))

    # Set the title and labels
    ax.set_title('Characteristics Contributing to Clusters')
    ax.set_xlabel('Count')
    ax.set_ylabel('Characteristics')

    # Rotate the x-axis labels
    plt.xticks(rotation=90)

    # Show the chart
    plt.tight_layout()
    file_name = f"DF{index} Contributing Char"
    plt.savefig(f"Results\DataSets\DataSet{index}\Contr_features\{file_name}_{method_name}_{key}.png")
    plt.show()
    
    
    ###########Graficos de todas las variables
    data= dataset.copy()
    # Cuenta los valores en la columna 'Cluster'
    cluster_counts = data['Cluster'].value_counts()

    # Crea una tabla din√°mica para sumar los valores de cada cluster
    table = pd.pivot_table(data, index=['Cluster'], aggfunc='sum')
    clusters = []
    minimum_index = 1000
    for i in range(len(cluster_counts)):
        cluster = table.iloc[i].sort_values(ascending=False)
        
        cluster = cluster.nlargest(np.count_nonzero(cluster))
        if minimum_index > np.count_nonzero(cluster):
            minimum_index = np.count_nonzero(cluster)
        clusters.append(cluster)

    # Crea una lista para guardar los √≠ndices de cada cluster
    indices = [cluster.index for cluster in clusters]
    # Usa la funci√≥n reduce de la biblioteca functools para encontrar la intersecci√≥n de todos los √≠ndices
    from functools import reduce
    common_index = reduce(lambda a, b: a.intersection(b), indices)
    # Crear un diccionario para almacenar los datos de cada cluster
    data_clusters = {}
    
    # Iterar sobre cada cluster
    for i in range(len(clusters)):
        # A√±adir los datos del cluster al diccionario
        data_clusters[f'Cluster{i}'] = clusters[i][clusters[i].index]#common_index - clusters[i].index
        
    
    # Crear un DataFrame con los datos de los clusters
    data_cluster = pd.DataFrame(data_clusters)
    # Obt√©n los nombres de las columnas (es decir, los nombres de los clusters)
    cluster_names = data_cluster.columns.tolist()
    
    # Ordena los datos en orden descendente
    data_cluster = data_cluster.sort_values(by=cluster_names, ascending=True)
    
    # Selecciona solo las primeras 40 filas
    data_cluster = data_cluster.iloc[:len(data_cluster)]
    
    # Ordena los datos en orden ascendente
    data_cluster = data_cluster.sort_values(by=cluster_names, ascending=True)
    
    plt.figure()
    # Create the stacked bar chart
    ax = data_cluster.plot.bar(stacked=True, figsize=(60, 25)) # Modificaci√≥n aqu√≠
    
    # Set the title and labels
    ax.set_title('Characteristics Contributing to Clusters')
    ax.set_ylabel('Characteristics') # Modificaci√≥n aqu√≠
    ax.set_xlabel('Count') # Modificaci√≥n aqu√≠
    
    # Rotate the x-axis labels
    plt.xticks(rotation=90)

    # Rotate the x-axis labels
    #plt.xticks(rotation=90)

    # Show the chart
    plt.tight_layout()
    file_name = f"DF{index} Full Features"
    plt.savefig(f"Results\DataSets\DataSet{index}\Full_features\{file_name}_{method_name}_{key}.png")
    plt.show()
    
    
# i=0
# for method, metrics in cluster_dataset_dictionary.items():
#     i=i+1
#     for metric, dataset in metrics.items():
#         if dataset is not None:
#             for key, df in dataset.items():
#                 if df is not None and 'Cluster' in df.columns:
#                     get_contrib_caracteristics(df, method, metric, key, i)
#                     df_original['Cluster'] = df['Cluster']
#                     df_original.to_csv(f"Results\DataSets\DataSet{i}\DataSet{i}_prediction_{metric}_{key}.csv")


get_contrib_caracteristics(dbscan_cluster3, "Dataset3", "DBSCAN", "", 3)
df_original['Cluster'] = dbscan_cluster3['Cluster']
df_original.to_csv("Results\DataSets\DataSet3\DataSet3_prediction_DBSCAN.csv")

get_contrib_caracteristics(dbscan_cluster2, "Dataset2", "DBSCAN", "", 2)
df_original['Cluster'] = dbscan_cluster2['Cluster']
df_original.to_csv("Results\DataSets\DataSet2\DataSet2_prediction_DBSCAN.csv")

get_contrib_caracteristics(dbscan_cluster1, "Dataset1", "DBSCAN", "", 1)
df_original['Cluster'] = dbscan_cluster1['Cluster']
df_original.to_csv("Results\DataSets\DataSet1\DataSet1_prediction_DBSCAN.csv")





def get_cluster_results(index, dataset_or, prediction_dictionary, cluster_dataset_dictionary):
    data = dataset_or.copy()
    
    ############################# Elbow #############################
    # kmeans Elbow
    data = dataset_or.copy()
    elbow = kmeans_elbow_method(index+1, data, 5)
    prediction_dictionary[f'dataset{index+1}']['kmeans']['elbow'] = elbow
    # kmeans Elbow cluster
    data = dataset_or.copy()
    dataset = kmeans_model(elbow, index+1, data, numerical_columns1, numerical_columns2)
    get_contrib_caracteristics(dataset, f'dataset{index+1}', 'kmeans', 'elbow', index+1)
    get_unique_caracteristics(dataset, f'dataset{index+1}', 'kmeans', 'elbow', index+1)
    dataset.to_csv(f"Results\DataSets\DataSet{index+1}\DataSet{index+1}_prediction_kmeans_elbow.csv")
    cluster_dataset_dictionary[f'dataset{index+1}']['kmeans']['elbow'] = dataset

    # kmodes Elbow
    data = dataset_or.copy()
    elbow = kmodes_elbow_method(index+1, data, 5)
    prediction_dictionary[f'dataset{index+1}']['kmodes']['elbow'] = elbow
    # kmodes Elbow cluster
    data = dataset_or.copy()
    dataset = kmodes_model(elbow, index+1, data, numerical_columns1, numerical_columns2)
    get_contrib_caracteristics(dataset, f'dataset{index+1}', 'kmodes', 'elbow', index+1)
    get_unique_caracteristics(dataset, f'dataset{index+1}', 'kmodes', 'elbow', index+1)
    dataset.to_csv(f"Results\DataSets\DataSet{index+1}\DataSet{index+1}_prediction_kmodes_elbow.csv")
    cluster_dataset_dictionary[f'dataset{index+1}']['kmodes']['elbow'] = dataset

    # kprototypes Elbow
    data = dataset_or.copy()
    elbow = kprototypes_elbow_method(index+1, data, numerical_columns1, numerical_columns2, 5)
    prediction_dictionary[f'dataset{index+1}']['kprototypes']['elbow'] = elbow
    # kprototypes Elbow cluster
    data = dataset_or.copy()
    dataset = kprototype_model(elbow, index+1, data, numerical_columns1, numerical_columns2)
    get_contrib_caracteristics(dataset, f'dataset{index+1}', 'kprototypes', 'elbow', index+1)
    get_unique_caracteristics(dataset, f'dataset{index+1}', 'kprototypes', 'elbow', index+1)
    dataset.to_csv(f"Results\DataSets\DataSet{index+1}\DataSet{index+1}_prediction_kprototypes_elbow.csv")
    cluster_dataset_dictionary[f'dataset{index+1}']['kprototypes']['elbow'] = dataset
    
    ############################# silhouette #############################
    # kmeans silhouette
    data = dataset_or.copy()
    silhouette = kmeans_silhouette_method(index+1, data, 5)
    prediction_dictionary[f'dataset{index+1}']['kmeans']['silhouette'] = silhouette
    # kmeans silhouette cluster
    data = dataset_or.copy()
    dataset = kmeans_model(silhouette, index+1, data, numerical_columns1, numerical_columns2)
    get_contrib_caracteristics(dataset, f'dataset{index+1}', 'kmeans', 'silhouette', index+1)
    get_unique_caracteristics(dataset, f'dataset{index+1}', 'kmeans', 'silhouette', index+1)
    dataset.to_csv(f"Results\DataSets\DataSet{index+1}\DataSet{index+1}_prediction_kmeans_silhouette.csv")
    cluster_dataset_dictionary[f'dataset{index+1}']['kmeans']['silhouette'] = dataset
    
    # kmodes silhouette
    data = dataset_or.copy()
    silhouette = kmodes_silhouette_method(index+1, data, 5)
    prediction_dictionary[f'dataset{index+1}']['kmodes']['silhouette'] = silhouette
    # kmodes silhouette cluster
    data = dataset_or.copy()
    dataset = kmodes_model(silhouette, index+1, data, numerical_columns1, numerical_columns2)
    get_contrib_caracteristics(dataset, f'dataset{index+1}', 'kmodes', 'silhouette', index+1)
    get_unique_caracteristics(dataset, f'dataset{index+1}', 'kmodes', 'silhouette', index+1)
    cluster_dataset_dictionary[f'dataset{index+1}']['kmodes']['silhouette'] = dataset
    
    # kprototypes silhouette
    data = dataset_or.copy()
    silhouette = kprototypes_silhouette_method(index+1, data, numerical_columns1, numerical_columns2, 5)
    prediction_dictionary[f'dataset{index+1}']['kprototypes']['silhouette'] = silhouette
    # kprototypes silhouette cluster
    data = dataset_or.copy()
    dataset = kprototype_model(silhouette, index+1, data, numerical_columns1, numerical_columns2)
    get_contrib_caracteristics(dataset, f'dataset{index+1}', 'kprototypes', 'silhouette', index+1)
    get_unique_caracteristics(dataset, f'dataset{index+1}', 'kprototypes', 'silhouette', index+1)
    cluster_dataset_dictionary[f'dataset{index+1}']['kprototypes']['silhouette'] = dataset
    
    ############################# Gap statist #############################
    data = dataset_or.copy()
    k, gapdf = kmeans_gap_method(data, 5)
    prediction_dictionary[f'dataset{index+1}']['kmodes']['gap_stat'] = k
    prediction_dictionary[f'dataset{index+1}']['kmeans']['gap_stat'] = k
    prediction_dictionary[f'dataset{index+1}']['kprototypes']['gap_stat'] = k
    
    
    dataset = kmodes_model(k, index+1, data, numerical_columns1, numerical_columns2)
    get_contrib_caracteristics(dataset, f'dataset{index+1}', 'kmeans', 'gap_stat', index+1)
    get_contrib_caracteristics(dataset, f'dataset{index+1}', 'kmodes', 'gap_stat', index+1)
    get_contrib_caracteristics(dataset, f'dataset{index+1}', 'kprototypes', 'gap_stat', index+1)
    get_unique_caracteristics(dataset, f'dataset{index+1}', 'kmeans', 'gap_stat', index+1)
    get_unique_caracteristics(dataset, f'dataset{index+1}', 'kmodes', 'gap_stat', index+1)
    get_unique_caracteristics(dataset, f'dataset{index+1}', 'kprototypes', 'gap_stat', index+1)
    cluster_dataset_dictionary[f'dataset{index+1}']['kmeans']['gap_stat'] = dataset
    cluster_dataset_dictionary[f'dataset{index+1}']['kmodes']['gap_stat'] = dataset
    cluster_dataset_dictionary[f'dataset{index+1}']['kprototypes']['gap_stat'] = dataset
    

get_cluster_results(0, dataset1.copy(), prediction_dictionary, cluster_dataset_dictionary)
get_cluster_results(1, dataset2.copy(), prediction_dictionary, cluster_dataset_dictionary)
get_cluster_results(2, dataset3.copy(), prediction_dictionary, cluster_dataset_dictionary)
    


i=0
for diccionario in prediction_dictionary:
    i+=1
    df = pd.DataFrame(prediction_dictionary[f"dataset{i}"])
    # Guardar el DataFrame en un archivo CSV o Excel
    nombre_archivo = f"Results\DataSets\DataSet{i}\Predictions\DF{i}_predictions.csv"  
    df.to_csv(nombre_archivo)


"""## 6. Visualization of results"""

end = time.time()
print(end - start)


############# nombre individual for cluster ###########
df1_kmeans_elbow = pd.read_csv("Results\DataSets\DataSet1\DataSet1_prediction_kmeans_elbow.csv")

df1_kmeans_elbow.set_index('Grave_ID', inplace=True)
n_individual_cluster_df1 = df1_kmeans_elbow.join(df_original["Individual"])
n_individual_cluster_df1 = n_individual_cluster_df1.groupby('Cluster')['Individual'].count()

############# nombre individual for cluster ###########
df1_kmeans_elbow = pd.read_csv("Results\DataSets\DataSet1\DataSet1_prediction_kmeans_elbow.csv")
df1_kmeans_elbow.set_index('Grave_ID', inplace=True)
print(df_original[["N_Parure", "Cluster"]])
n_nparure_cluster_df1 = df1_kmeans_elbow.join(df_original[["N_Parure","Individual"]])
n_nparure_cluster_df1 = n_nparure_cluster_df1.groupby('N_Parure')['Individual'].count()



df_original[["Individual", "Type"]]
count = df_original.groupby('Type')['Individual'].count()




import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import time
import hashlib
import scipy
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs




############################### Hierarichal #####################################
"""import scipy.cluster.hierarchy as sch
import matplotlib.pyplot as plt

X = dataset.values

A = sch.single(dataset)
B = sch.complete(dataset)
C = sch.average(dataset)

linkage_matrices = [A, B, C]
methods = ['single', 'complete', 'average']

for i, Z in enumerate(linkage_matrices):
    plt.figure(figsize=(20,10))
    dendrogram = sch.dendrogram(Z)
    plt.title('Dendrogram - ' + methods[i])
    plt.xlabel('Tombs Index')
    plt.ylabel('Euclidean distances')
    nombre = "Hierarichal_" + methods[i]
    fecha = datetime.datetime.now().strftime("%Y_%m_%d")
    plt.savefig(f"{nombre}_{fecha}.png", dpi=300)
    plt.show()
    """
"""
import matplotlib.pyplot as mp
import sklearn
cutree = sch.cut_tree(A)
data_columns = X.columns
labels = list([i[0] for i in cut])
labeled_data = pd.DataFrame(X, columns=data_columns)
labeled_data['label'] = labels

fig, axes = mp.subplots(nrows = 1, 
                        ncols = 1, 
                        figsize = (4,4), 
                        dpi=300)
sklearn.tree.plot_tree(model,
                       feature_names = X.columns,
                       filled = True,
                       class_names=True);
"""
"""Four different samples will be created:
- Full burial sample (the full dataframe)
- Top sub-level only
- Lower sublevel only
- Data from new excavations"""

#Top level data only:
#top = df.loc[df['Sub-Level'] == '0.50']
#low = df.loc[df['Sub-Level'] == '1.00']
#new=

#samples = [top, low, new]
