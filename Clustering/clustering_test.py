# -*- coding: utf-8 -*-
"""Clustering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16qB3zPedR-NDYXbbjEHJZ8jHWshlWQcF

# 1.Library Import
"""

# Commented out IPython magic to ensure Python compatibility.
#import os
# proxy is not needed for other computers
#os.environ['http_proxy'] = 'http://proxy-chain.intel.com:911'
#os.environ['https_proxy'] = 'http://proxy-chain.intel.com:912'

#!pip install openpyxl
#!pip3 install xgboost
#!pip install -U imbalanced-learn
#!pip install gensim
#!pip install nltk
#import nltk
#nltk.download('stopwords')
#nltk.download('punkt')
#!pip install sklearn
#!pip install langdetect

# Import required libraries
import numpy as np
import re #
import pandas as pd
# %matplotlib inline
import warnings


# Import required libraries for machine learning classifiers and NLP
from sklearn.preprocessing import OneHotEncoder
enc = OneHotEncoder(handle_unknown='ignore')
#try:
  #from catboost import CatBoostClassifier
#except:
  #!pip install catboost
  #from catboost import CatBoostClassifier

#!pip install kmodes
from kmodes.kmodes import KModes
import matplotlib.pyplot as plt
# %matplotlib inline



# Set random seed
np.random.seed(42)

warnings.filterwarnings("ignore")

#Measure run time
import time
start = time.time()

"""## 2. Data Upload"""

#Upload Safar's list of 193 tombs
#The source of the data must be changed according to where it is stored.
###################################################################################################
file = pd.read_csv(r'SafarTombs.csv')
###################################################################################################

#Create a copy of the data
df = file
len(df)

"""## 3. Feature Engineering
In this section, the original data is split and reconfigured in an adequate format for the model to ingest.
This includes:
- Data cleaning and wrangling
- Renaming of columns when needed
- Splitting columns with large descriptions into individual categories
- Selection of variables for analysis
- One-hot-encoding
- Creation of additional variables, including total number of burial items, total number of pottery types represented
"""
def clean_text(text):
    print(text)
    if pd.isna(text):
        return text
    # Remove extra whitespace
    clean_text = ' '.join(text.split())

    # Remove line breaks and other special control characters
    clean_text = clean_text.replace('\n', ' ')
    clean_text = clean_text.replace('\r', '')
    clean_text = clean_text.lower()
    clean_text = clean_text.strip()
    return clean_text

#Creates unique body ID for burials with multiple bodies
df['ID'] = df['Grave'].astype(str) + df['Individual']
df['Grave_ID'] = df['Grave'].astype(str)+"_"+df['ID'].astype(str)
df.set_index('Grave_ID', inplace=True)

#Creates variable for burial type
df['Type'] = df['Type'].str.lower()
df['Burial Type'] = pd.np.where(df['Type'].str.contains("libn box"), "Libn box", df['Type'])
df['Burial Type'] = pd.np.where(df['Type'].str.contains("floor"), "Libn floor", df['Burial Type'])
df['Burial Type'] = pd.np.where(df['Type'].str.contains("shaft"), "Shaft tomb", df['Burial Type'])
df['Burial Type'] = pd.np.where(df['Type'].str.contains("sand"), "In sand", df['Burial Type'])
df['Burial Type'] = pd.np.where(df['Type'].str.contains("debris"), "In debris", df['Burial Type'])
df.loc[pd.isnull(df['Type']), 'Burial Type'] = np.nan

#Clean size information
df['Size'] = df['Size'].replace('c.', "", regex=True).astype(float)

def category_body(text):
    words_to_replace = {
        ######################### body #########################################
        'extended on back': [
            'extended on back',
            'extended on the back',
            'extended',
            'skeleton extended on back',
            'body laid on back',
            'body extended',
            'body on back',
            'extended on back',
            'on back',
            'originally extended on back',
            'originally extended',
            '',
            'body on female back'
        ],
        'body to the right': [
            'body slightly turned on right side',
            'body to the right',
            'extended slightly on the right side',
            'body placed on the right side',
            'body slightly on the right',
            'body extended on right side',
            'extended on right side',
            'body placed on right side'
        ],
        'body to the left': [
            'body slightly turned on left side',
            'body to the left',
            'body on left side',
            'extended slightly on the left side',
            'body placed on the left side',
            'body slightly on the left',
            'body extended on left side',
            'extended on left side',
            'body on the left side',
            'body placed on left side'
        ],
    
   ######################### hands #########################################
       'hands near pelvis': [
           'hands on legs',
           'hands below pelvis',
           'hands on pelvis',
           'hands near pelvis',
       ],
       'hands beside legs': [
           'hands extended',
           'hands beside legs',
           'hands by the sides',
           'hands at side',
       ],
       'hands crossed on pelvis': [
           'hands one over another near the pelvis',
           'hands meeting at the pelvis',
           'hands by the sides',
       ],
       'hands upwards': [
           'upwards hands'
       ],
       'hand near chin': [
           'right hand near chin',
           'left hand touching chin'
       ],
       'right hand near pelvis': [
           'right hand on pelvis',
           'right hand near pelvis',
           'right hand flexed on pelvis',
       ],
       'left hand near pelvis': [
           'left hand on pelvis',
           'left hand near pelvis',
           'left hand on pelvis orientaded nw',
       ],
       'left hand beside leg': [
           'left hand extended',
           'left hand at side',
       ],
       'right hand beside leg': [
           'right hand extended',
           'right hand at side',
       ],
       'hand near pelvis': [
           'hand near pelvis',
       ],
       ######################### face #########################################
        'face upward': [
            'face',
            'face upward',
        ],
        'face eastward': [
            'face eastward',
            'face slightly eastward',
            'face slightly eastwards',
        ],
        'face westward': [
            'face westward',
            'face slightly westward',
            'face slightly westwards',
        ],
        ######################### skull #########################################
        'skull collapsed': [
            'skull collapsed',
            'skull crushed',
            'skull fallen',
            'skull smashed',
            'head collapsed',
            'skull collapsed, displaced',
        ],
        'skull missing': [
            'skull missing',
            'head missing',
        ],
        'skull to the right': [
            'skull found on the right side of this burial',
            'head slightly westward',
        ],
        'skull to the left': [
            'skull found on the left to the female skeleton',
            'skull east to the left arm',
        ],
        ######################### skull #########################################
        'legs flexed': [
            'legs slightly bent',
            'legs slightly in desorder',
            'legs flexed',
            'right slightly in flexed',
            'legs bent backward',
            'legs flexed at knees',
            'legs bent westward',
        ],
        'legs crossed': [
            'legs crossed',
        ],
        'legs extended': [
            'legs extended',
            'legs extended but not in line with the spine',
        ],
        'legs missing': [
            'legs missing due to pit cut',
        ],
       }
    if pd.isna(text):
        return text

    array_text = text.split(';')
    modified_array = []

    for item in array_text:
        item_lower = item.lower()
        replaced = False
        for category, words in words_to_replace.items():
            for word in words:
                if item_lower == word.lower():
                    modified_array.append(category)
                    replaced = True
                    break  # Salir del bucle interno si hay una coincidencia
            if replaced:
                break  # Salir del bucle externo si se hizo una sustitución
        else:
            modified_array.append(item)

    joined_text = ';'.join(modified_array)

    return joined_text

df['Position'] = df['Position'].apply(category_body)
df['Position']

# Split Body position in separate phrases
def sentences(text):
    delimiters= ";"
    text = re.split(delimiters, str(text))
    clean_sent = []
    for sent in text:
        clean_sent.append(sent)
    return clean_sent

# sentence parsing creation
df1 = df['Position'].apply(sentences)
df1 = df['Position'].str.split(';', expand=True)
df1.columns = ['text1','text2','text3','text4','text5']

#Finds mentions of body position and copies text to appropiate columnn
############Find more pythonic way of doing this###########################################
names = ["body", "extended", "embryonic"]
df1['body'] = pd.np.where(df1['text1'].str.contains("|".join(names)), df1['text1'], np.nan)
df1['body'] = pd.np.where(df1['text2'].str.contains("|".join(names)), df1['text2'], df1['body'])
df1['body'] = pd.np.where(df1['text3'].str.contains("|".join(names)), df1['text3'], df1['body'])
df1['body'] = pd.np.where(df1['text4'].str.contains("|".join(names)), df1['text4'], df1['body'])
df1['body'] = pd.np.where(df1['text5'].str.contains("|".join(names)), df1['text5'], df1['body'])

#Finds mentions of hands and copies text to appropiate columnn
############Find more pythonic way of doing this###########################################
df1['hands'] = pd.np.where(df1['text1'].str.contains("hand"), df1['text1'], np.nan)
df1['hands'] = pd.np.where(df1['text2'].str.contains("hand"), df1['text2'], df1['hands'])
df1['hands'] = pd.np.where(df1['text3'].str.contains("hand"), df1['text3'], df1['hands'])
df1['hands'] = pd.np.where(df1['text4'].str.contains("hand"), df1['text4'], df1['hands'])
df1['hands'] = pd.np.where(df1['text5'].str.contains("hand"), df1['text5'], df1['hands'])

#Finds mentions of face and copies text to appropiate columnn
############Find more pythonic way of doing this###########################################
df1['face'] = pd.np.where(df1['text1'].str.contains("face"), df1['text1'], np.nan)
df1['face'] = pd.np.where(df1['text2'].str.contains("face"), df1['text2'], df1['face'])
df1['face'] = pd.np.where(df1['text3'].str.contains("face"), df1['text3'], df1['face'])
df1['face'] = pd.np.where(df1['text4'].str.contains("face"), df1['text4'], df1['face'])
df1['face'] = pd.np.where(df1['text5'].str.contains("face"), df1['text5'], df1['face'])

#Finds mentions of arms and copies text to appropiate columnn
############Find more pythonic way of doing this###########################################
df1['arm'] = pd.np.where(df1['text1'].str.contains("arm"), df1['text1'], np.nan)
df1['arm'] = pd.np.where(df1['text2'].str.contains("arm"), df1['text2'], df1['arm'])
df1['arm'] = pd.np.where(df1['text3'].str.contains("arm"), df1['text3'], df1['arm'])
df1['arm'] = pd.np.where(df1['text4'].str.contains("arm"), df1['text4'], df1['arm'])
df1['arm'] = pd.np.where(df1['text5'].str.contains("arm"), df1['text5'], df1['arm'])

#Finds mentions of skulls and copies text to appropiate columnn
############Find more pythonic way of doing this###########################################
names = ["skull", "head"]
df1['skull'] = pd.np.where(df1['text1'].str.contains("|".join(names)), df1['text1'], np.nan)
df1['skull'] = pd.np.where(df1['text2'].str.contains("|".join(names)), df1['text2'], df1['skull'])
df1['skull'] = pd.np.where(df1['text3'].str.contains("|".join(names)), df1['text3'], df1['skull'])
df1['skull'] = pd.np.where(df1['text4'].str.contains("|".join(names)), df1['text4'], df1['skull'])
df1['skull'] = pd.np.where(df1['text5'].str.contains("|".join(names)), df1['text5'], df1['skull'])

#Finds mentions of legs and copies text to appropiate columnn
############Find more pythonic way of doing this###########################################
names = ["leg", "knee"]
df1['leg'] = pd.np.where(df1['text1'].str.contains("|".join(names)), df1['text1'], np.nan)
df1['leg'] = pd.np.where(df1['text2'].str.contains("|".join(names)), df1['text2'], df1['leg'])
df1['leg'] = pd.np.where(df1['text3'].str.contains("|".join(names)), df1['text3'], df1['leg'])
df1['leg'] = pd.np.where(df1['text4'].str.contains("|".join(names)), df1['text4'], df1['leg'])
df1['leg'] = pd.np.where(df1['text5'].str.contains("|".join(names)), df1['text5'], df1['leg'])

df = pd.concat([df, df1[['body','hands','face','arm','skull','leg']]], axis=1).drop(['Position'], axis=1)

df.columns

#Separates multiple items into dummies with unique pottery types
df2 = df[['Pottery types']]

patron1 = r'\b\d{1,3}[A-Za-z]\b'
patron2 = r'\b\d{2}\b'

# Combine patron1 and patron2 using the | symbol for "or"
patron = f'{patron1}|{patron2}'
finds_match = []
other_objects_match = []

# Iterar a través de la columna 'Texto'
for texto in df2['Pottery types']:
    if pd.isna(texto):
        finds_match.append(texto)
        other_objects_match.append(texto)
    else:
        coincidencias_finds = re.findall(patron, texto)
        texto_modificado = re.sub(patron, '', texto)
        texto_modificado = re.sub("pottery types:", '', texto_modificado)
        texto_modificado = re.sub("pottery types", '', texto_modificado)
        texto_modificado = re.sub(r'[^\w\s]', '', texto_modificado)
        other_objects_match.append(texto_modificado)
        if len(coincidencias_finds) != 0:
            finds_match.append(';'.join(coincidencias_finds))
        else:
            finds_match.append(float('nan'))
        
df2["Pottery types"] = finds_match



df2=df2.join(df[['Level']])
df2=df2.set_index('Level')

df2 = df2['Pottery types'].str.get_dummies(sep=';')
df2.columns = df2.columns.str.strip()
#reverse dummies into columns
df2 = df2.replace(1, pd.Series(df2.columns, df2.columns))
df2 = df2.applymap(lambda x: 1 if isinstance(x, str) else x)

df_pottery=df2.join(df[['Level']])
df_pottery.to_csv("pottery_sample.csv")

#Separates multiple items into dummies with unique body positions and burial types
df3 = df[['body','hands','face','arm','skull','leg', 'Burial Type']]
#One-Hot-Encoding for body positions
df4 = pd.get_dummies(data=df3, columns=['body','hands','face','arm','skull','leg', 'Burial Type'])
df4.columns = df4.columns.str.strip()
#reverse dummies into columns
df4 = df4.replace(1, pd.Series(df4.columns, df4.columns))

# Join all datasets
dataset = df2.join(df4).join(df[['Grave', 'Individual', 'Level', 'Orientation', 'Type', 'Sex', 'Age',
       'Size', 'Other objects']])
dataset = dataset.replace(np.nan, '').astype(str)

dataset

"""Four different samples will be created:
- Full burial sample (the full dataframe)
- Top sub-level only
- Lower sublevel only
- Data from new excavations"""

#Top level data only:
#top = df.loc[df['Sub-Level'] == '0.50']
#low = df.loc[df['Sub-Level'] == '1.00']
#new=

#samples = [top, low, new]

"""## 4. Clustering Models
Focused on algorithms specific to categorical data or mixed data

## K-Modes
"""

data =  dataset.copy()
# Elbow curve to find optimal K - I tried both with random init and Cao init
cost = []
K = range(1,5)
for num_clusters in list(K):
    kmode = KModes(n_clusters=num_clusters, init = "random", n_init = 5, verbose=1)
    kmode.fit_predict(data)
    cost.append(kmode.cost_)

plt.plot(K, cost, 'bx-')
plt.xlabel('No. of clusters')
plt.ylabel('Cost')
plt.title('Elbow Method For Optimal k')
plt.show()

"""Configuration of all the models to test and application to each sample"""
# Building the model with 2 clusters
kmode = KModes(n_clusters=2, init = "random", n_init = 5, verbose=1)
clusters = kmode.fit_predict(data)

"""Definition of optimal number of clusters with loop for each sample"""
#for x in samples:

data.insert(0, "Cluster", clusters, True)

#Prepare for evaluation of feature importance
cols = df2.columns
data[cols] = np.where(data[cols] =='0', 0, 1)

cols2 = df4.columns
data[cols2] = np.where(data[cols2] =='0', 0, 1)

len(data)

data['Cluster'].value_counts()

#reverse dummies into columns
table = pd.pivot_table(data, index=['Cluster'], aggfunc='sum')
cluster0 = table.iloc[0]
cluster0 = cluster0.sort_values(ascending=False)
cluster1 = table.iloc[1]
cluster1 = cluster1.sort_values(ascending=False)

diff0 = cluster0.nlargest(50).index.difference(cluster1.nlargest(30).index)
diff1 = cluster1.nlargest(30).index.difference(cluster0.nlargest(30).index)

diff0

#, facecolor='w', edgecolor='k')
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 5))
fig.suptitle('Top-40 contributing features to clusters')
cluster0.nlargest(50).plot(ax=ax1, kind='bar')
cluster1.nlargest(50).plot(ax=ax2, kind='bar')
ax1.title.set_text('Cluster 0 Unique characteristics')
ax2.title.set_text('Cluster 1 Unique characteristics')

"""## K-Prototypes
First published by Huang (1998)
"""

#data[]

#Preprocessing numerical
numerical = data.select_dtypes(exclude='object')
numerical

"""## 6. Visualization of results"""

end = time.time()
print(end - start)